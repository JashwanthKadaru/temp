[
  {
    "title": "Model Documentation Template",
    "text": "Template Owner: Model Risk Management Template Last Updated: July. 2024 Template Statement Model Owners are ultimately responsible for ensuring that model development and maintenance is properly documented—comprehensive and transparent enough that the model development process can be replicated by a knowledgeable third party. Model Owners often employ the assistance of other parties, such as the Model Developers and the Model Implementation Team, when writing the model development documentation. This template is designed to aid in the model development and maintenance documentation process for all models. The Model Owner should use judgment to determine if additional detail or elements not explicitly identified in this template should also be incorporated. The addition or removal of sections should be done at the discretion of the Model Owner, based on relevance to the model at hand and in accordance with the Model Risk Management Procedures. Guidelines (indicated in blue text throughout the document) have been provided to guide the writer regarding desired contents and writing style. Please note that these are merely suggestions and should only be included if applicable. Documentation Scope The documentation requirements may vary depending on the model, for example depending on whether the model is internally- or externally-developed, as well as statistical or computational. Unless otherwise mentioned, this documentation requirement applies to all models. Complete the table below to record the history of changes to this document, as well as the update of version number. Complete a new row of the table for each major revision to the document, typically occurring if the model itself has changed, or the documentation has been modified. Judgment should be applied to determine whether a change is major or minor (i.e., V1.1 or V2.0, respectively). For the first draft of the model documentation, please list as ‘initial version’ with version number 1.0. Contents Model Documentation Template\t1 1.\tDocument Introduction\t5 1.1\tPurpose\t5 1.2\tScope\t5 1.3\tTerms and Descriptions\t5 2.\tExecutive Summary\t6 2.1\tBusiness Context and Requirements\t6 2.2\tObjective of the Model\t6 2.3\tIntended Use and Scope of Applicability\t6 2.4\tModel Revenue Information\t6 2.5\tUpstream/Downstream Models\t6 2.6\tModel Overview\t6 2.7\tRegulatory/Compliance/Legal Requirements\t6 2.8\tModel History\t7 2.9\tModel Ongoing Change Plan\t7 3.\tDevelopment Data\t8 3.1\tDevelopment Data Set Overview\t8 3.2\tData Extraction\t8 3.3\tData Reconciliation and Quality\t8 3.4\tData Cleansing\t9 3.5\tData Sampling\t9 3.6\tDevelopment Data Testing\t9 3.7\tDevelopment Data and Processing/Testing Code\t9 4.\tModel Methodology\t11 4.1\tModel Theory and Approach\t11 4.1.1\tModel Framework Overview\t11 4.1.2\tModel Segmentation\t11 4.1.3\tModel Processing Logic\t11 4.2\tModel Features/Variables\t11 4.3\tModel Assumptions/Limitations\t12 4.4\tExpert Consultation and Industry Research (if applicable)\t13 4.5\tModel Methodology Testing\t13 4.6\tModel Specification\t13 4.7\tModel Development and Testing Code\t14 5.\tModel Output\t15 5.1\tSensitivity, Stability and Robustness Testing\t15 5.2\tCalibration and Overlay Testing\t15 5.3\tBenchmark Testing\t15 5.4\tBias Testing\t15 6.\tModel Implementation\t16 6.1\tModel Implementation Processes and Procedures\t16 6.2\tProduction Input Data\t16 6.3\tModel Implementation Testing\t16 6.4\tUpdating Production Model\t17 7.\tModel Ongoing Monitoring\t18 7.1\tOngoing Monitoring\t18 7.2\tBusiness Continuity\t19 7.3\tModel Access Management\t19 8.\tAppendix\t20 8.1\tSupplemental Documents\t20 8.2\tOther\t20 ",
    "subsections": []
  },
  {
    "title": "Document Introduction",
    "text": "",
    "subsections": [
      {
        "title": "Purpose",
        "text": "Describe the purpose of this document. "
      },
      {
        "title": "Scope",
        "text": "Describe the scope/coverage of this document. "
      },
      {
        "title": "Terms and Descriptions",
        "text": "Describe all acronyms used in this document. "
      }
    ]
  },
  {
    "title": "Executive Summary",
    "text": "",
    "subsections": [
      {
        "title": "Business Context and Requirements",
        "text": "Briefly describe the context of the business that uses the model. This should include Visa’s businesses as well as any applicable clients. Describe the sources of business requirements and the process of gathering business requirements. Also, describe the coordination between all relevant functions to ensure the model meets the business requirements. Please include a summary of the business requirements in this section and include the actual business requirements documents in appendix. "
      },
      {
        "title": "Objective of the Model",
        "text": "Briefly describe the objective of the model (e.g. fraud detection, credit rating, etc.) "
      },
      {
        "title": "Intended Use and Scope of Applicability",
        "text": "Briefly describe the intended uses of the model including how the model is intended to serve the line(s) of business, geography, and product(s) to which the model can be applied. Include whether the model is used for internal purposes, or the model / its output are provided to Visa’s clients. The scale of the model usage, including risk exposure and the model usage (transaction counts per day or per month, credit exposure of the transactions). "
      },
      {
        "title": "Model Revenue Information",
        "text": "For all revenue-generating models, the potential expected annual direct revenue of the model must be provided. It is also strongly recommended to provide the potential expected annual indirect revenue, if applicable. "
      },
      {
        "title": "Upstream/Downstream Models",
        "text": "List all upstream models, whose outputs are used as the inputs of this model. List any dependent models as per the knowledge of the model owner or the model development team. If there is not any upstream model, please specify no upstream model. If there is not any downstream model, please specify no downstream model. "
      },
      {
        "title": "Model Overview",
        "text": "Provide a brief overview of the model, including model development history and model design. Describe the process for the model development including key stakeholders, roles and responsibilities of the key stakeholders, governance of the development process (e.g. committees), timeline, etc. "
      },
      {
        "title": "Regulatory/Compliance/Legal Requirements",
        "text": "Describe the pertinent regulatory or compliance requirements and the impact of these matters (e.g., Data Use Case Assessment (DUCA), Privacy Risk Assessment (PRA)), if applicable. Provide evidence of legal approval, if applicable. "
      },
      {
        "title": "Model History",
        "text": "Provide a high-level description of history of changes and the timing for the changes, if possible, include the records for all previous versions. For the most recent change, provide description include details of changes made in, for example, model segmentations, model features and modeling methods. Please also explain the rationale of related changes, if applicable. "
      },
      {
        "title": "Model Ongoing Change Plan",
        "text": "Describe any pre-scheduled change plan for future versions of model development, which is scheduled/seasonal model change regardless of model performance evaluation, if applicable. "
      }
    ]
  },
  {
    "title": "Development Data",
    "text": "",
    "subsections": [
      {
        "title": "Development Data Set Overview",
        "text": "For each data input: Provide a name for each data input source. Provide a description of key data characteristics, including data structure (e.g., time series, panel data, unstructured, or structured tabular, etc.), level of granularity (transaction-level, account-level or merchant-level etc.), length of data available, length of data considered for modeling (and kept as part of testing), and rationale for choice of data versus other potential options (with a description of alternatives considered). If certain regulations require the use of a particular data type, describe those requirements, and discuss how the selected development dataset complies with the requirements, or explain why these requirements cannot be met. Describe the data sources used, including internal vs. external, frequency of data updates to collect data from these sources (e.g., quarterly, monthly vs. weekly). In selecting data sources and data elements for inclusion in the modeling process, consideration is given to the: Applicability of the data to the model purpose Integrity and accuracy of historical data Expected future availability and reliability of the systems providing the data. Expected future integrity and accuracy of the data across all of the channels and segments to which the model will be applied. Compliance with applicable business and regulatory requirements - As a general rule, internal data must be used whenever possible. If internal data are not available, or external data are necessary to fill gaps in the data, it is especially critical to assess data quality, suitability, and relevance to the model for the external data. If official centralized Company data sources cannot be utilized and alternative data sources are utilized instead, describe the controls must be in place to ensure data integrity. "
      },
      {
        "title": "Data Extraction",
        "text": "Describe procedures performed to collect data from these sources. Include information on how the security and confidentiality of customer information was ensured, and where the development code is centralized, if applicable. "
      },
      {
        "title": "Data Reconciliation and Quality",
        "text": "Quantitative and qualitative analysis and checks should be performed to assess the quality of data, especially as it relates to accuracy, completeness, stability, alignment with strategy described above, outlier analysis, missing values, consistency across multiple data sources, and ability to pass certain logical tests. Elements of this section may include description of: Data quality checks, controls, and procedures applied to ensure data accuracy and completeness. Review of frequency distributions of data values, high, and low values (to ensure consistency with business intuition) – Include detailed tables with variable statistics in the Appendix (as applicable) Review of unusual values and outliers – Describe the rationale and adjustment approach if any adjustments or overrides were applied; include measurements of the impact of excluding/adjusting outliers. Review of period-over-period changes in key data elements – Discuss if there are any unusual variances identified and how these potential integrity concerns were resolved. Manual checks to ensure automated checks in place function appropriately. "
      },
      {
        "title": "Data Cleansing",
        "text": "Details of any applicable scrubbing rules (merging, cleaning, filtering, and segmenting) applied to the source data, along with empirical support for the use of any missing/replace data values performed during the creation of a relevant and complete dataset for modeling. Any exclusions (e.g., exclusions for missing values, indeterminates and low-quality labels, etc.) should be clearly stated and justified. Waterfall charts are recommended for data exclusions. "
      },
      {
        "title": "Data Sampling",
        "text": "Detail the sampling procedures/methodology, including the impact of the sampling specification on the model estimation (e.g. sample weighting) used during the creation of the development data sets. Additionally, include the rationale, process for splitting and time periods of training, testing and Out-of-Time validation of the development data, if applicable. "
      },
      {
        "title": "Development Data Testing",
        "text": "Describe and summarize the tests that were performed on development data, for example (but not limited to), data exploratory analysis, overlaying histograms, seasonality testing and testing on missing value imputations. Draw conclusions/opine on each testing results. Include testing results in the documentation and specify the location of the code for all tests conducted. "
      },
      {
        "title": "Development Data and Processing/Testing Code",
        "text": "Provide a link to the centralized location where the data processing/testing code are saved, and the location where the final development data is saved.  Code should be clearly labeled and documented to support reproduction by someone other than the model developer. Please include the following: Link to data and code. Final data testing code [Required for reproducing testing results] Data sampling and/or cohort creation code Data engineering code used to create variables. Data engineering code used to create the training, out-of-sample and out-of-time datasets. Any other relevant documentation Please note that it is the development team’s responsibility that the development code is executable, if given the same environment setting. "
      }
    ]
  },
  {
    "title": "Model Methodology",
    "text": "Document the model estimation technique/methodology (e.g. theory, formulas, etc.) including support for any deviations away from standard estimation techniques and evidence that model approach is reasonable and consistent with business requirements and user specifications. For instance, the reasoning behind choosing a particular methodology for model development, its alignment with the business objectives, any industry and academic research supporting the use of this methodology for similar objectives, and/or any constraints that hinder the development team from employing other potentially superior methodologies. Include selected functional form/statistical distribution/simulation method and consider using diagrams to illustrate complex statistical concepts discussed. ",
    "subsections": [
      {
        "title": "Model Theory and Approach",
        "text": "Descriptions of each of the model functional specifications, theoretical structure, components, data flows, and assumptions/variables. Model Framework Overview Include an overview of the model architecture including a high-level process flow diagram of the model. Model Segmentation Describe the model’s segmentations schema, if any. Include the rationale for the selection of the model segmentation (i.e., business justification, difference in characteristics and importance ranking of key features in each segmentation). Describe any alternative segmentation schema that has been considered. Document the test scripts, and results for validating the accuracy of model components. Model Processing Logic Describe the model’s components or sub-modules. Include their relation to the intended usage (i.e., how the output of the component is utilized) and rationale for the selection of the model structure. Detail the process flow diagram of the model framework and descriptions of each component. Document the test scripts, and results for validating the accuracy of model components. "
      },
      {
        "title": "Model Features/Variables",
        "text": "Document and identify for all model variables/parameters a description, creation rule, source data field(s), format, decoding rules, etc. Ensure all variables/parameters are documented and support provided for their reasonability/consistency relative to model objectives/specifications. Support should include: Starting long list of considered variables, describe the categories of variables considered and the criteria guiding their selection. Step-by-step process and criteria for narrowing down the long list. Include tables summarizing the various steps in the variable selection process. Transformations applied to the variables, including the rationale for the selected transformations. If any statistical software was applied to select optimal variable transformations, describe the underlying procedures in detail. Any procedures applied to derive the considered variables (e.g., calculation of ratios from raw data inputs) Provide data dictionary that includes the name, description, and calculation logic for derived features. Please be sure to specify both the target variables (dependent variable(s) being solved for, if any) and the input variables (independent variables that the model/function is pulling in). "
      },
      {
        "title": "Model Assumptions/Limitations",
        "text": "Document the key model assumptions/limitations that are considered for developing and utilizing the model. Include support for the reasonability/consistency with applicable business requirements or similar industry models. For limitations, indicate any remediation actions taken. List all assumptions made in the model in the table below. Where applicable, link this to mitigants and controls in place Note: Technically, all analytical assumptions need to be tested. Expert Assumptions are assumptions made based on domain knowledge, experience, and intuition of data scientists or industry experts. For example, ‘the labels provided by issuers/acquires/merchants are trustworthy and adequately represent the objective reality’, or ‘the pattern of transactions identified as fraudulent in the past will be somewhat similar in the future’. Analytical Assumptions are assumptions made about the structure or properties of the model itself or the underlying data. For example, the assumptions about the distribution of data, the independence of features, or the form of the relationship between input variables and the target variable. List all limitations (shortcomings that cannot reasonably be remediated by the Model Owner and Model Developer) of the methodology in the table below including a clear description of the limitation. Where applicable, link this to mitigants and controls in place "
      },
      {
        "title": "Expert Consultation and Industry Research (if applicable)",
        "text": "Describe supporting insights from experts regarding the reasonability of the proposed model approach/design. Describe any relevant industry literature supporting and evaluating the reasonability of the proposed model approach/design. Include any meeting minutes form discussions with business experts and research publications in the Appendix. "
      },
      {
        "title": "Model Methodology Testing",
        "text": "Provide the results of the tests conducted for the final model. Assumption Testing For each assumption listed in section 4.5, conclude whether the assumption is reasonable and how well this is evidenced. If the assumption is an analytical assumption, describe any tests performed on these assumptions. Model Components Testing Detail any testing results conducted for each model components (e.g., performance, stability, or seasonality). Model components may encompass aspects such as, preprocessing models for determining optimal segmentations or sample weights, encoding/embedding models employed during the feature engineering process, and seasonality models utilized for determine seasonal factors. Model Performance Testing Detail the final model performance by providing the values for all select metrics for model performance evaluation. Description of the procedure for hyperparameter tuning process and provide evidence (this can be logs in the development code provided) that the final selected set of hyperparameters produces the optimal model performance, if applicable. List the final set of hyperparameters/architectures of the model. Alternative Model Testing Provide results for performance evaluation for all or selected alternative models as evidence that the final champion model outperforms all its alternatives. Describe the process and rationale of selecting the final champion model/methodology from all candidate/alternative models. "
      },
      {
        "title": "Model Specification",
        "text": "Describe the functional structure of the model along with identifying essential features and their associated parameters. If the model's structure and parameters cannot be distinctly outlined, alternate interpretation methods such as Partial Dependence Plots, SHAP, or LIME can be deployed. When dealing with a large feature set, focus on the key features. Similarly, when the data volume is substantial, it is acceptable to use a representative sample for analysis. "
      },
      {
        "title": "Model Development and Testing Code",
        "text": "Provide a link to the centralized location where model development code is saved.  Code should be clearly labeled and documented to support reproduction by someone other than the model developer. Please include the following: Link to code. Official documented scoring code for ongoing use [Required to avoid version control issues] Final development & scoring code used in validation [Required for reproducing testing results] Final model testing code [Required for reproducing testing results] Any other relevant documentation "
      }
    ]
  },
  {
    "title": "Model Output",
    "text": "",
    "subsections": [
      {
        "title": "Sensitivity, Stability and Robustness Testing",
        "text": "Detail results for model performance testing on the development data out-of-time, as well as segmentation analysis to assess model performance stability across various segments (i.e., region, funding source, card presented or not, etc.). "
      },
      {
        "title": "Calibration and Overlay Testing",
        "text": "Describe the methodology and parameters for any post-model calibrations and/or overlays. Provide rationales for transforming the raw model output to the final ready-in-production output. Details the evaluation of potential impact on any post-model calibrations and/or overlays. Describe ongoing change plans for determining calibration and/or overlay parameters, if applicable. Include step-by-step process for any adjustments to raw model output (i.e., by Model Owner, business unit representative, etc.). "
      },
      {
        "title": "Benchmark Testing",
        "text": "Detail the results for benchmark testing. Provide evidence that the final champion model exhibits superior or equivalent performance when compared to any benchmark models, upstream models or current production model. For updated versions of models that are currently in production, a benchmark comparison with the refitted version (keeping everything identical but refitting with more recent data) of the existing model is strongly recommended, unless an adequate justification is presented (e.g. the updated model involves a scope expansion or production data is not retrievable). "
      },
      {
        "title": "Bias Testing",
        "text": "Describe any bias testing conducted. Detail the process of evaluating if the final model is unfairly favoring certain categories or groups over others. This could be based on attributes such as gender, race, income, or any other characteristic, if related testing is conducted. "
      }
    ]
  },
  {
    "title": "Model Implementation",
    "text": "",
    "subsections": [
      {
        "title": "Model Implementation Processes and Procedures",
        "text": "Describe in detail the model’s application and configuration specifications including – technical platform specifications, sequencing, modules, reporting, etc. Describe the process for putting the model into production, platform etc. For example, if the model is implemented in an incremental manner, describe any analysis conducted to ensure there are no major issues as a result of the new model upload. Describe where the production fields are stored. Include formal process / procedures for maintaining and updating key modeling parameters, assumptions, data sources, computer code and parties responsible for approving the changes including individuals allowed to request a change, documentation requirements (e.g. change log), approvals, implementing and testing a change etc. "
      },
      {
        "title": "Production Input Data",
        "text": "Describe source, control and management plan for data feeding into the model in production. "
      },
      {
        "title": "Model Implementation Testing",
        "text": "Describe the implementation testing results to demonstrate that the model is developed and validated is the same model that is implemented. Implementation testing should include both input and output testing results. Provide evidence as well as results for implementation testing as follows. Detail the results of input implementation testing below. If the list for input features is long, list a group of key features’ testing results. Detail the results of output implementation testing below. "
      },
      {
        "title": "Updating Production Model",
        "text": "Describe system performance testing conducted as well as any plans in place to ensure production continuity in the event of system failures or implementation failure. "
      }
    ]
  },
  {
    "title": "Model Ongoing Monitoring",
    "text": "",
    "subsections": [
      {
        "title": "Ongoing Monitoring",
        "text": "Detail the ongoing performance monitoring process including the measurement metrics, acceptable ranges, and remediation actions when score distributions are not aligned. Also include reference documents in appendix, if any. Example for monitoring plan is as follows: "
      },
      {
        "title": "Business Continuity",
        "text": "Describe any plans in place to ensure business continuity in the event of model failure. "
      },
      {
        "title": "Model Access Management",
        "text": "Describe how access to the production is managed. "
      }
    ]
  },
  {
    "title": "Appendix",
    "text": "The Appendix should include, but is not limited to, the following: Detailed data integrity results. Complete list of and description of input variables considered. Additional supporting exhibits and detailed model development results. Meeting minutes from discussions with business experts. Implementation testing results for input. Model code location. ",
    "subsections": [
      {
        "title": "Supplemental Documents",
        "text": ""
      },
      {
        "title": "Other",
        "text": ""
      }
    ]
  }
]